[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "CV",
    "section": "",
    "text": "Etiam congue quam eget velit convallis, eu sagittis orci vestibulum. Vestibulum at massa turpis. Curabitur ornare ex sed purus vulputate, vitae porta augue rhoncus. Phasellus auctor suscipit purus, vel ultricies nunc. Nunc eleifend nulla ac purus volutpat, id fringilla felis aliquet. Duis vitae porttitor nibh, in rhoncus risus. Vestibulum a est vitae est tristique vehicula. Proin mollis justo id est tempus hendrerit. Praesent suscipit placerat congue. Aliquam eu elit gravida, consequat augue non, ultricies sapien. Nunc ultricies viverra ante, sit amet vehicula ante volutpat id. Etiam tempus purus vitae tellus mollis viverra. Donec at ornare mauris. Aliquam sodales hendrerit ornare. Suspendisse accumsan lacinia sapien, sit amet imperdiet dui molestie ut."
  },
  {
    "objectID": "blog/second-post/index.html",
    "href": "blog/second-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula. Amet cursus sit amet dictum sit amet. Eget duis at tellus at urna condimentum. Convallis aenean et tortor at risus viverra. Tincidunt ornare massa eget egestas purus viverra accumsan. Et malesuada fames ac turpis egestas. At imperdiet dui accumsan sit amet. Ut ornare lectus sit amet est placerat. Enim nulla aliquet porttitor lacus luctus accumsan tortor posuere. Duis ultricies lacus sed turpis tincidunt id aliquet risus. Mattis enim ut tellus elementum sagittis. Dui id ornare arcu odio ut. Natoque penatibus et magnis dis. Libero justo laoreet sit amet cursus sit. Sed faucibus turpis in eu. Tempus iaculis urna id volutpat lacus laoreet.\nPhasellus vestibulum lorem sed risus. Eget felis eget nunc lobortis mattis. Sit amet aliquam id diam maecenas ultricies. Egestas maecenas pharetra convallis posuere morbi. Etiam erat velit scelerisque in dictum non consectetur a erat. Cras fermentum odio eu feugiat pretium nibh ipsum consequat. Viverra accumsan in nisl nisi scelerisque. Et netus et malesuada fames ac. Amet tellus cras adipiscing enim eu turpis egestas pretium aenean. Eget lorem dolor sed viverra ipsum nunc aliquet. Ultrices dui sapien eget mi proin sed libero enim sed. Ultricies mi eget mauris pharetra et ultrices neque. Ipsum suspendisse ultrices gravida dictum. A arcu cursus vitae congue mauris rhoncus aenean vel. Gravida arcu ac tortor dignissim convallis. Nulla posuere sollicitudin aliquam ultrices."
  },
  {
    "objectID": "blog/post-20250710/index.html",
    "href": "blog/post-20250710/index.html",
    "title": "Introduction BLS CPI data",
    "section": "",
    "text": "The landing page for the Consumer Price Index (CPI) is https://www.bls.gov/cpi/. To access the underlying data, select “Databases” within the drop down menu of the “CPI Data” tab to open https://www.bls.gov/cpi/data.htm .\nI am interested in the “All Urban consumers (Current Series)”, so I clicked on the text icon on the right-hand side which opens the directory https://download.bls.gov/pub/time.series/cu/\nThe “cu.txt” file contains information on the structure of the CPI code. The CPI code consists of five elements:\n\nsurvey abbreviation, e.g. “CU” represents urban consumers\n\nseasonal (code), “S” denotes seasonally adjusted, “R” denotes seasonally unadjusted. These are summarized in the “cu.seasonal” text file.\n\nperiodicity code: “R” denotes monthly and “S” denotes semi-annual\n\narea_code: “0000” denotes “U.S. city average” and other codes are listed in “cu.area”text file.\nitem_code : “SAO” denotes “All items” and other codes are listed in item “cu.item” text file.\n\nThe file “cu.series” contains a taxonomy of all the CPI indexes that includes the five elements list above, the series title and additional information about the start- and end-date of the series,\nBefore exploring more of the files in https://download.bls.gov/pub/time.series/cu/ we will review the relative importance page to gain a better understanding of some of the key components of the CPI. One can access this page starting at https://www.bls.gov/cpi/ and selecting “Tables” in the “CPI Data” dropdown menu, and then clicking on the link under “Relative importance and weights”.\nDownload the xlsx file under the “Relative Importance” heading to obtain the weights of the sub-indexes within the “All items” index. The worksheet labelled “Table 1” contains the weights in the urban consumer CPI (CPI-U) and the clerical workers CPI (CPI-W). The first column lists the level of aggregation of the index that runs from level 1 (the most aggregated) to level 8 (the least aggregated). Of the 322 indexes, we will ignore the 27 indexes listed under “special aggregate indexes” and review the level 1 indexes that remain. The eight level 1 indexes and their weights are listed in the table below\n\n\n\nRelative importance of Level 1 CPI indexes\n\n\nThe weights sum to 100 for both the CPI-U and CPI-W indexes.\nReturning to https://download.bls.gov/pub/time.series/cu/ you will notice that each of the level 1 indexes has a file that contains data. For example, “cu.11.USFoodBeverage” contains the time series data of all the indexes within the Food and beverage level 1 CPI index. The data is in the tidy format and includes the series id, year, period, value and footnote codes. The period is defined in the cu.period text file."
  },
  {
    "objectID": "blog/post-20250710/index.html#introduction",
    "href": "blog/post-20250710/index.html#introduction",
    "title": "Introduction BLS CPI data",
    "section": "",
    "text": "The landing page for the Consumer Price Index (CPI) is https://www.bls.gov/cpi/. To access the underlying data, select “Databases” within the drop down menu of the “CPI Data” tab to open https://www.bls.gov/cpi/data.htm .\nI am interested in the “All Urban consumers (Current Series)”, so I clicked on the text icon on the right-hand side which opens the directory https://download.bls.gov/pub/time.series/cu/\nThe “cu.txt” file contains information on the structure of the CPI code. The CPI code consists of five elements:\n\nsurvey abbreviation, e.g. “CU” represents urban consumers\n\nseasonal (code), “S” denotes seasonally adjusted, “R” denotes seasonally unadjusted. These are summarized in the “cu.seasonal” text file.\n\nperiodicity code: “R” denotes monthly and “S” denotes semi-annual\n\narea_code: “0000” denotes “U.S. city average” and other codes are listed in “cu.area”text file.\nitem_code : “SAO” denotes “All items” and other codes are listed in item “cu.item” text file.\n\nThe file “cu.series” contains a taxonomy of all the CPI indexes that includes the five elements list above, the series title and additional information about the start- and end-date of the series,\nBefore exploring more of the files in https://download.bls.gov/pub/time.series/cu/ we will review the relative importance page to gain a better understanding of some of the key components of the CPI. One can access this page starting at https://www.bls.gov/cpi/ and selecting “Tables” in the “CPI Data” dropdown menu, and then clicking on the link under “Relative importance and weights”.\nDownload the xlsx file under the “Relative Importance” heading to obtain the weights of the sub-indexes within the “All items” index. The worksheet labelled “Table 1” contains the weights in the urban consumer CPI (CPI-U) and the clerical workers CPI (CPI-W). The first column lists the level of aggregation of the index that runs from level 1 (the most aggregated) to level 8 (the least aggregated). Of the 322 indexes, we will ignore the 27 indexes listed under “special aggregate indexes” and review the level 1 indexes that remain. The eight level 1 indexes and their weights are listed in the table below\n\n\n\nRelative importance of Level 1 CPI indexes\n\n\nThe weights sum to 100 for both the CPI-U and CPI-W indexes.\nReturning to https://download.bls.gov/pub/time.series/cu/ you will notice that each of the level 1 indexes has a file that contains data. For example, “cu.11.USFoodBeverage” contains the time series data of all the indexes within the Food and beverage level 1 CPI index. The data is in the tidy format and includes the series id, year, period, value and footnote codes. The period is defined in the cu.period text file."
  },
  {
    "objectID": "blog/post-20250710/index.html#adding-descriptions-to-the-cpi-data",
    "href": "blog/post-20250710/index.html#adding-descriptions-to-the-cpi-data",
    "title": "Introduction BLS CPI data",
    "section": "Adding descriptions to the CPI data",
    "text": "Adding descriptions to the CPI data\nFor the purposes of automating report creation, I tried to use the files available on the BLS website without much preprocessing outside of R. The tab-delimited cu.series.txt file is very useful, but I cannot load it without running into an error so I have created a csv version that works better. The three most useful variables are: series_id, area_code and series_title. I want to focus on the seasonally adjusted U.S. urban consumer price index so I need to select the series_id that begins with “CU”, select the area_code that equals 0 , and the seasonal code that equals “S”.\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\npath &lt;- \"G:\\\\My Drive\\\\Research\\\\Inflation\\\\\"\n\ndfs &lt;- read.csv(paste0(path,\"cu.series.csv\"))\n\ndf &lt;- dfs[grepl(\"CU\",dfs$series_id),]\n\ndf &lt;- df[df$area_code == \"0\",]\n\ndf &lt;- df[df$seasonal == \"S\",]\n\ndf$short_name &lt;- str_replace(df$series_title,\" in U.S. city average, all urban consumers, seasonally adjusted\",\"\")\n\nhead(df[,c(\"series_id\",\"short_name\")])\n\n\n\n#Read in Apparel CPI and calculate annual percentage change\n\ndA &lt;- read.table(paste0(path,\"cu.data.13.USApparel.txt\"), sep=\"\\t\", header = TRUE)\n\ndA$Date &lt;- as.Date(paste0(dA$year,\"-\",substr(dA$period,2,3),\"-01\"),format = \"%Y-%m-%d\")\n\ndA &lt;- dA[grepl(\"CUS\",dA$series_id),]\n\ndA &lt;- dA %&gt;%   arrange(series_id,Date) %&gt;%\n   mutate(\n    percentage_change = (value - lag(value,n=12)) / lag(value,n=12) * 100\n  )\n\ndA &lt;- merge(dA,df[,c(\"series_id\",\"short_name\")], by.x=\"series_id\",by.y=\"series_id\",all.x=TRUE)\n\ndB &lt;- dA[dA$Date &gt; as.Date(\"2000-01-01\"),]\n\nvar &lt;- unique(dB$series_id)\n\nfiglist &lt;- list()\n\nfor(kv in (1:length(var))){\n  \n  figlist[[kv]] &lt;- ggplot(data = dB[dB$series_id==var[kv],],\n                          mapping = aes(x=Date,y=percentage_change )) +\n    geom_line() +\n      theme_light() + \n    theme(plot.background = element_blank(),\n          panel.background = element_blank(),\n          axis.title.x = element_blank(),\n          legend.position = \"none\",\n          plot.title = element_text(hjust = 0.5)) +\n    ylab(\"Annual Inflation\") +\n    ggtitle(dB[dB$series_id==var[kv],]$short_name) + \n    geom_hline(yintercept=2, linetype=\"dashed\",\n               color = \"red\", linewidth=1)\n  \n}\n\nggarrange(figlist[[1]],figlist[[2]],figlist[[3]],figlist[[4]],nrow=2,ncol=2)"
  },
  {
    "objectID": "blog/post-20250710/index.html#miscellaneous",
    "href": "blog/post-20250710/index.html#miscellaneous",
    "title": "Introduction BLS CPI data",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nitem descriptions Each item code is defined."
  },
  {
    "objectID": "blog/post-20250504/index.html",
    "href": "blog/post-20250504/index.html",
    "title": "Trade Data",
    "section": "",
    "text": "The Census Bureau has an API for downloading trade data. There are five components to the API:\n\nThe base url\nThe choice of imports or exports\nThe endpoint, i.e. one of nine commodity classification systems.\nThe variables you want to download\nThe filter\n\nLet’s begin with a simple example. What are the US exports of Soybeans in the most recent FT900 trade report?\nThe base url is “https://api.census.gov/data/timeseries/intltrade” and then there are four levels of information\n\nWe are choosing “exports”\nWe are using the “enduse” classification system for the ft900 report\nThe variables are:\n\n\n“E_ENDUSE_SDESC” - a short description of the commodity\n“ALL_VAL_MO” - the dollar amount exported\n“CTY_CODE” - the country code for the soy exports\n“CTY_NAME” - the country name for the exports\n\n\nThe filter\n\n\n“E_ENDUSE=111110” - the enduse code for soybeans\n“time=2025-03” - the year and month\n\n\nbase &lt;- \"https://api.census.gov/data/timeseries/intltrade\"\n\nl1 &lt;- \"exports\"\nl2 &lt;- \"enduse\"\nl3 &lt;- \"E_ENDUSE_SDESC,ALL_VAL_MO,CTY_CODE,CTY_NAME\"\nl4 &lt;- \"time=2025-03&E_ENDUSE=111110\"\n\napi_call &lt;- paste0(base,l1,\"/\",l2,\"?get=\",l3,\"&\",l4)\n\nprint(api_call)\n\nThe api_call has the following url:\nhttps://api.census.gov/data/timeseries/intltrade/exports/enduse?get=E_ENDUSE_SDESC,ALL_VAL_MO,CTY_CODE,CTY_NAME&time=2025-03&E_ENDUSE=111110\nThe api call can be pasted into a browser to download the data, but it is easier to read into R and create a data frame.\n\nlibrary (httr)\nlibrary (jsonlite)\n\nres &lt;- GET(api_call)\n\nx &lt;-rawToChar(res$content)\n\npredf &lt;- fromJSON(rawToChar(res$content))\n\ndf &lt;- as.data.frame(predf[2:nrow(predf), ])\nnames(df) &lt;- predf[1, ]\n\nhead(df)"
  },
  {
    "objectID": "blog/first-post/index.html",
    "href": "blog/first-post/index.html",
    "title": "First Post",
    "section": "",
    "text": "Sed risus ultricies tristique nulla aliquet. Neque volutpat ac tincidunt vitae semper quis lectus nulla.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Enim sed faucibus turpis in eu mi bibendum neque. Ac orci phasellus egestas tellus rutrum tellus pellentesque eu. Velit sed ullamcorper morbi tincidunt ornare massa. Sagittis id consectetur purus ut faucibus pulvinar elementum integer. Tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada proin libero. Lobortis feugiat vivamus at augue eget arcu. Aliquam ut porttitor leo a diam sollicitudin tempor id eu. Mauris a diam maecenas sed enim ut sem viverra aliquet. Enim ut tellus elementum sagittis vitae et leo duis. Molestie at elementum eu facilisis sed odio morbi quis commodo. Sapien pellentesque habitant morbi tristique senectus. Quam vulputate dignissim suspendisse in est. Nulla pellentesque dignissim enim sit amet venenatis urna cursus eget.\nVelit aliquet sagittis id consectetur purus ut faucibus pulvinar elementum. Viverra mauris in aliquam sem fringilla ut morbi tincidunt augue. Tortor at auctor urna nunc id. Sit amet consectetur adipiscing elit duis tristique sollicitudin. Aliquet nibh praesent tristique magna sit amet purus. Tristique senectus et netus et malesuada fames ac turpis. Hac habitasse platea dictumst quisque. Auctor neque vitae tempus quam pellentesque nec nam aliquam. Ultrices tincidunt arcu non sodales neque sodales ut etiam. Iaculis at erat pellentesque adipiscing. Cras tincidunt lobortis feugiat vivamus. Nisi est sit amet facilisis magna etiam. Pharetra pharetra massa massa ultricies mi quis hendrerit. Vitae sapien pellentesque habitant morbi tristique senectus. Ornare aenean euismod elementum nisi quis eleifend quam adipiscing vitae."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I was born and raised in South Africa and completed a B.Sc(Hons) (Statistics) and MA (Economics) at the University of Cape Town. After spending two years at Simon Fraser University, I transferred to the University of Rochester where I completed a Phd in Economics (Econometrics, Finance). I was a tenure track Assistant Professor at Emory University for five years before moving to New York City to work in the Financial Services Industry. I spent 10 years building prepay and default models for non-agency MBS at Citi and AllianceBernstein followed by 10 years in quantitative risk management at TIAA."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 12, 2025\n\n\nComitting project to github from RStudio\n\n\nOwen \n\n\n\n\nMay 22, 2021\n\n\nFirst Post\n\n\nAlicia \n\n\n\n\nJul 10, 2025\n\n\nIntroduction BLS CPI data\n\n\nOwen \n\n\n\n\nMay 23, 2021\n\n\nSecond Post\n\n\nAlicia \n\n\n\n\nMay 24, 2021\n\n\nThird Blog Post\n\n\nAlicia \n\n\n\n\nMay 4, 2025\n\n\nTrade Data\n\n\nOwen \n\n\n\n\nMay 4, 2025\n\n\nTrade Data\n\n\nOwen \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/post-20250515/index.html",
    "href": "blog/post-20250515/index.html",
    "title": "Trade Data",
    "section": "",
    "text": "Let’s test the api within the blog\n\nlibrary (httr)\n\nWarning: package 'httr' was built under R version 4.3.3\n\nlibrary (jsonlite)\n\nWarning: package 'jsonlite' was built under R version 4.3.3\n\n#library (tidyverse)\n#library (ggplot2)\n\nbase &lt;- \"https://api.census.gov/data/timeseries/intltrade/\"\n\nl1 &lt;- \"imports\"\nl2 &lt;- \"hs\"\nl3 &lt;- \"I_COMMODITY_SDESC,GEN_VAL_YR,CTY_CODE,CTY_NAME\"\nl4 &lt;- \"time=from+2024-01&I_COMMODITY=87150*\"\n\napi_call &lt;- paste0(base,l1,\"/\",l2,\"?get=\",l3,\"&\",l4)\n\nres &lt;- GET(api_call)\n\npredf &lt;- fromJSON(rawToChar(res$content))\n\ndf &lt;- as.data.frame(predf[2:nrow(predf), ])\nnames(df) &lt;- predf[1, ]\n\ndf$GEN_VAL_YR &lt;- as.numeric(df$GEN_VAL_YR)\n\nhead(df)\n\n                                 I_COMMODITY_SDESC GEN_VAL_YR CTY_CODE\n1 BABY CARRIAGES (INC STROLLERS) AND PARTS THEREOF   35654928        -\n2 BABY CARRIAGES (INC STROLLERS) AND PARTS THEREOF     183241     0021\n3 BABY CARRIAGES (INC STROLLERS) AND PARTS THEREOF     131842     0003\n4 BABY CARRIAGES (INC STROLLERS) AND PARTS THEREOF   35337098     0014\n5 BABY CARRIAGES (INC STROLLERS) AND PARTS THEREOF     183241     0020\n6 BABY CARRIAGES (INC STROLLERS) AND PARTS THEREOF     318051     0022\n                         CTY_NAME    time I_COMMODITY\n1         TOTAL FOR ALL COUNTRIES 2024-01      871500\n2 TWENTY LATIN AMERICAN REPUBLICS 2024-01      871500\n3                  EUROPEAN UNION 2024-01      871500\n4           PACIFIC RIM COUNTRIES 2024-01      871500\n5                   USMCA (NAFTA) 2024-01      871500\n6                            OECD 2024-01      871500"
  },
  {
    "objectID": "blog/post-20250712/Index.html",
    "href": "blog/post-20250712/Index.html",
    "title": "Comitting project to github from RStudio",
    "section": "",
    "text": "Introduction\nI have struggled to blog regularly. As a consequence, I run into technology issues. On July 10th I wrote a blog and used the “Environment, Histor, ..” pane to commit my changes to git hub, but I received the following error\n\n\n\nCommit Error\n\n\nI shelved the decision to commit and waited until Saturday morning to learn more about gpg. I found the github pages to set up the key etc. and then I turned to RStudio to figure out how to add the key. A google search lead me to the fact that RStudio is set up for ssh and not gpg. I then set up a new ssh key within RStudio using the Tools &gt; Global Options &gt; Git/SVN &gt; Create SSH Key steps.\n\n\n\nCreate ssh key\n\n\nWhat I also remembered is that when I watched a video on setting up quarto for blogging, commits were done via the terminal window. So what worked for me is the following two lines at the command line:\ngit commit -m “commit message”\ngit push origin main\nwhere you specify the relevant “commit message”.\nIt’s actually quite simple if you know what you are doing, and waste countless hours if you don’t."
  },
  {
    "objectID": "blog/third-post/index.html",
    "href": "blog/third-post/index.html",
    "title": "Third Blog Post",
    "section": "",
    "text": "The source for any page in your website could also be a Jupyter Notebook. This one is third-post/index.ipynb.\nHere’s an example I borrowed from the Seaborn docs:\n\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\n# Load the diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# Plot the distribution of clarity ratings, conditional on carat\nsns.displot(\n    data=diamonds,\n    x=\"carat\", hue=\"cut\",\n    kind=\"kde\", height=4, aspect=1.5,\n    multiple=\"fill\", clip=(0, None),\n    palette=\"ch:rot=-.25,hue=1,light=.75\",   \n)"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Predicting House Prices with Machine Learning\n\n\n\nPython\n\n\nMachine Learning\n\n\nData Cleaning\n\n\n\nThis project involves using machine learning algorithms to predict house prices based on various features such as location, size, and amenities. It includes data cleaning, feature engineering, and model selection.\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Segmentation Using Clustering Techniques\n\n\n\nR\n\n\nMachine Learning\n\n\nClustering\n\n\nStatistical Modelling\n\n\n\nThis project focuses on segmenting customers into different groups based on their purchasing behavior and demographics. It uses clustering algorithms like K-means and hierarchical clustering to identify distinct customer segments.\n\n\n\nApr 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Global CO2 Emissions\n\n\n\nR\n\n\nData Visualization\n\n\nEnvironmental Science\n\n\n\nThis project involves creating visualizations to show trends in global CO2 emissions over time. It includes data extraction from public databases, data cleaning, and using visualization libraries to create interactive charts and graphs.\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]